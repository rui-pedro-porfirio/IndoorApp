{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wi-Fi Fingerprinting Experiments (Alternative Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Standard Library\n",
    "import getopt\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# IPython\n",
    "from IPython.display import display\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 10000)\n",
    "pd.set_option(\"display.max_columns\", 10000)\n",
    "\n",
    "# Matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt                       \n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "\n",
    "# NumPy\n",
    "import numpy as np                                    \n",
    "\n",
    "# SciPy\n",
    "import scipy as sp\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# StatsModels\n",
    "import statsmodels.api as sm\n",
    "\n",
    " # scikit-learn\n",
    "import sklearn                                       \n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_plots(results, save_to=None, figsize=(8, 8)):\n",
    "    fig, axarr = plt.subplots(2, 1, figsize=figsize)\n",
    "    \n",
    "    for key, result in results.items():\n",
    "        max_error = math.ceil(result[\"error\"].max())\n",
    "        kde = gaussian_kde(result[\"error\"].values)\n",
    "        X_plot=np.linspace(0, max_error, 1000)\n",
    "        axarr[0].plot(X_plot, kde.evaluate(X_plot), \"-\", label=key)\n",
    "    \n",
    "    axarr[0].set_xlabel(\"Error (e) in meters (m)\")\n",
    "    axarr[0].set_ylabel(r\"$F_X(e)$\")\n",
    "    axarr[0].xaxis.set_major_locator(MultipleLocator(0.5))\n",
    "    axarr[0].set_xlim(0, result[\"error\"].quantile(q=0.9975))\n",
    "    axarr[0].legend()\n",
    "\n",
    "    for key, result in results.items():\n",
    "        ecdf = sm.distributions.ECDF(result[\"error\"])\n",
    "        x = np.linspace(min(result[\"error\"]), max(result[\"error\"]))\n",
    "        y = ecdf(x)\n",
    "        axarr[1].plot(x, y, label=key)\n",
    "    \n",
    "    axarr[1].set_xlabel(\"Error (e) in meters (m)\")\n",
    "    axarr[1].set_ylabel(r\"$f_X(e)$\")\n",
    "    axarr[1].xaxis.set_major_locator(MultipleLocator(0.5))\n",
    "    axarr[1].yaxis.set_major_locator(MultipleLocator(0.1))\n",
    "    axarr[1].set_xlim(0, result[\"error\"].quantile(q=0.9975))\n",
    "    axarr[1].set_ylim(0)\n",
    "    axarr[1].legend()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    if save_to is not None:\n",
    "        fig.savefig(output_data_directory+\"/\"+save_to, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def experiment_statistics(result):\n",
    "    statistics = collections.OrderedDict([\n",
    "        (\"mae\",  result[\"error\"].abs().mean()),\n",
    "        (\"rmse\", np.sqrt((result[\"error\"]**2).mean())),\n",
    "        (\"sd\",   result[\"error\"].std()),\n",
    "        (\"p50\",  result[\"error\"].quantile(q=0.50)),\n",
    "        (\"p75\",  result[\"error\"].quantile(q=0.75)),\n",
    "        (\"p90\",  result[\"error\"].quantile(q=0.90)),\n",
    "        (\"p95\",  result[\"error\"].quantile(q=0.95)),\n",
    "        (\"min\",  result[\"error\"].min()),\n",
    "        (\"max\",  result[\"error\"].max()),\n",
    "    ])    \n",
    "    return statistics\n",
    "\n",
    "def knn_experiment(data, test_data, train_cols, coord_cols,\n",
    "                   scaler=None, n_neighbors=5, weights=\"uniform\",\n",
    "                   algorithm=\"auto\", leaf_size=30, p=2, metric=\"minkowski\",\n",
    "                   metric_params=None, n_jobs=1, leave_out=False):\n",
    "    result = None\n",
    "    knn = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm,\n",
    "                              leaf_size=leaf_size, p=p, metric=metric,\n",
    "                              metric_params=metric_params, n_jobs=n_jobs)\n",
    "    if scaler is not None:\n",
    "        estimator = make_pipeline(scaler, knn)\n",
    "    else:\n",
    "        estimator = knn\n",
    "    locations = data.groupby(coord_cols).indices.keys()\n",
    "    for coords in locations:\n",
    "        if leave_out is True:\n",
    "            train_data = data[(data[coord_cols[0]] != coords[0]) |\n",
    "                                  (data[coord_cols[1]] != coords[1])].reset_index(drop=True)\n",
    "        else:\n",
    "            train_data = data.copy().reset_index(drop=True)\n",
    "        target_values = test_data[(test_data[coord_cols[0]] == coords[0]) &\n",
    "                                      (test_data[coord_cols[1]] == coords[1])].reset_index(drop=True)\n",
    "            \n",
    "        estimator.fit(train_data[train_cols], train_data[coord_cols])\n",
    "        predictions = pd.DataFrame(estimator.predict(target_values[train_cols]), columns=coord_cols)\n",
    "        curr_result = target_values[coord_cols].join(predictions, rsuffix=\"_predicted\")\n",
    "        error = pd.DataFrame((predictions[coord_cols] - curr_result[coord_cols]).apply(np.linalg.norm, axis=1),\n",
    "                             columns=[\"error\"])\n",
    "        curr_result = pd.concat([curr_result, error], axis=1)\n",
    "        result = pd.concat([result, curr_result])\n",
    "    return result\n",
    "\n",
    "def knn_experiment_cv(data, cross_validation, train_cols, coord_cols,    \n",
    "                      scaler=None, n_neighbors=5, weights='uniform',\n",
    "                      algorithm=\"auto\", leaf_size=30, p=2, metric=\"minkowski\",\n",
    "                      metric_params=None, n_jobs=1):    \n",
    "    result = None\n",
    "    knn = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm,\n",
    "                              leaf_size=leaf_size, p=p, metric=metric,\n",
    "                              metric_params=metric_params, n_jobs=n_jobs)\n",
    "    if scaler is not None:\n",
    "        estimator = make_pipeline(scaler, knn)\n",
    "    else:\n",
    "        estimator = knn\n",
    "    X = data[train_cols]\n",
    "    y = data[coord_cols]\n",
    "    predictions = pd.DataFrame(cross_val_predict(estimator, X, y, cv=cross_validation), columns=coord_cols)\n",
    "    result = y.join(predictions, rsuffix=\"_predicted\")\n",
    "    error = pd.DataFrame((predictions[coord_cols] - result[coord_cols]).apply(np.linalg.norm, axis=1), columns=[\"error\"])\n",
    "    result = pd.concat([result, error], axis=1)    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class responsible for loading a JSON file (or all the JSON files in a given directory) into a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yanux.cruncher.model.loader import JsonLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class that takes a set of Python dictionaries containing Wi-Fi logging data loaded from JSON files collected by the YanuX Scavenger Android application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yanux.cruncher.model.wifi import WifiLogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Input & Output Data Directories and other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_directory = \"data\"\n",
    "output_data_directory = \"out\"\n",
    "statistics_excel_writer = pd.ExcelWriter(output_data_directory+\"/alt-statistics.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the output directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_data_directory):\n",
    "    os.makedirs(output_data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from the Input Data Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all files from the *data* folder.\n",
    "The logs currently placed there were collected using the **Yanux Scavenger** Android application on April 28<sup>th</sup>, 2016 using an LG Nexus 5 running Androdid Marshmallow 6.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_loader = JsonLoader(input_data_directory+\"/wifi-fingerprints\")\n",
    "wifi_logs = WifiLogs(json_loader.json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wi-Fi Readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Recorded Samples per Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_location = int(len(wifi_logs.wifi_samples()) / len(wifi_logs.locations))\n",
    "num_samples_per_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the data into a Pandas Dataframe, in which each Wi-Fi result reading is represented by a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wifi_results_columns = [\"filename\", \"place\", \"floor\", \"x\", \"y\", \"orientation\", \"sample_id\", \"mac_address\",\n",
    "                        \"timestamp\", \"signal_strength\"]\n",
    "\n",
    "wifi_results = pd.DataFrame(wifi_logs.wifi_results(), columns=wifi_results_columns)\n",
    "wifi_results.to_csv(output_data_directory + \"/alt-wifi_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the unique MAC Addresses present in the recorded data. Each one represents a single Wi-Fi Access Point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac_addresses = wifi_results.mac_address.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, store the data into a Pandas Dataframe in which each line represents a single sampling cycle with *n* different readings for each of the Access Points within range. Those readings are stored as columns along each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wifi_samples_columns = [\"filename\", \"place\", \"floor\", \"x\", \"y\", \"orientation\", \"sample_id\", \"timestamp\"]\n",
    "wifi_samples_columns.extend(mac_addresses)\n",
    "\n",
    "wifi_samples = pd.DataFrame(wifi_logs.wifi_samples(), columns=wifi_samples_columns)\n",
    "wifi_samples = wifi_samples.sort_values([\"filename\", \"x\", \"y\", \"floor\", \"sample_id\"]).reset_index(drop=True)\n",
    "wifi_samples.to_csv(output_data_directory + \"/alt-wifi_samples.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wifi_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Unique Mac Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(wifi_results.mac_address.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How often has each Access Point been detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wifi_results_mac_address_group = wifi_results.groupby(\"mac_address\")\n",
    "wifi_results_mac_address_group.size().plot(kind=\"bar\")\n",
    "wifi_results_mac_address_group.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wifi_results_mac_address_group.size().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many Wi-Fi results were gathered at each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wifi_results_coord_group = wifi_results.groupby([\"x\", \"y\"])\n",
    "wifi_results_coord_group.size().plot(kind=\"bar\")\n",
    "wifi_results_coord_group.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wifi_results_coord_group.size().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many APs were detected at each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wifi_ap_per_location = wifi_samples.groupby([\"x\",\"y\"]).min()[wifi_results_mac_address_group.size().keys()].count(axis=1)\n",
    "wifi_ap_per_location.plot(kind=\"bar\")\n",
    "wifi_ap_per_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wifi_ap_per_location.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The coordinates of the points where data was captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coords = wifi_results[[\"x\",\"y\"]].drop_duplicates().sort_values(by=[\"x\",\"y\"]).reset_index(drop=True)\n",
    "coords_plot_size = (min(coords[\"x\"].min(),coords[\"y\"].min()), max(coords[\"x\"].max(),coords[\"y\"].max()))\n",
    "#TODO: If I end up using it in the document, then I should refactor the plot to use matplotlib directly to tweak a few things.\n",
    "coords.plot(figsize=(16,5), x=\"x\",y=\"y\", style=\"o\", grid=True, legend=False,\n",
    "            xlim=coords_plot_size, ylim=coords_plot_size,\n",
    "            xticks=np.arange(coords_plot_size[0]-1, coords_plot_size[1]+1, 1),\n",
    "            yticks=np.arange(coords_plot_size[0]-1, coords_plot_size[1]+1, 1)).axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Strength Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wifi_results.hist(column=\"signal_strength\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a train and test scenario to be used by default when testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = mac_addresses\n",
    "coord_cols = [\"x\",\"y\"]\n",
    "\n",
    "full_data_scenario = wifi_samples.copy()\n",
    "full_data_scenario_groups = full_data_scenario[\"x\"].map(str)+\",\"+full_data_scenario[\"y\"].map(str)+\",\"+full_data_scenario[\"filename\"].map(str)\n",
    "\n",
    "default_data_scenario, test_data_scenario, default_data_scenario_groups, test_data_scenario_groups=train_test_split(full_data_scenario, full_data_scenario_groups, test_size=0.3, random_state=42)\n",
    "\n",
    "default_data_scenario = default_data_scenario.copy().reset_index(drop=True)\n",
    "test_data_scenario = test_data_scenario.copy().reset_index(drop=True)\n",
    "default_data_scenario_groups = default_data_scenario_groups.copy().reset_index(drop=True)\n",
    "test_data_scenario_groups = test_data_scenario_groups.copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_n_neighbors=5\n",
    "default_weights=\"distance\"\n",
    "default_metric=\"manhattan\"\n",
    "default_nan_filler=default_data_scenario[mac_addresses].min().min()*1.010\n",
    "default_scaler=preprocessing.MinMaxScaler()\n",
    "default_leave_out=True\n",
    "\n",
    "DefaultCrossValidator = StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors=default_n_neighbors\n",
    "weights=default_weights\n",
    "metric=default_metric\n",
    "nan_filler=default_nan_filler\n",
    "scaler=default_scaler\n",
    "leave_out=default_leave_out\n",
    "\n",
    "curr_data = default_data_scenario.fillna(nan_filler)\n",
    "test_data = test_data_scenario.fillna(nan_filler)\n",
    "\n",
    "curr_result = knn_experiment(curr_data,\n",
    "                             test_data,\n",
    "                             mac_addresses,\n",
    "                             coord_cols,\n",
    "                             scaler=scaler,\n",
    "                             algorithm=\"brute\",\n",
    "                             n_neighbors=n_neighbors,\n",
    "                             weights=weights,\n",
    "                             metric=metric,\n",
    "                             leave_out=leave_out)\n",
    "\n",
    "curr_statistics = experiment_statistics(curr_result)    \n",
    "curr_result.to_csv(output_data_directory+\"/alt-results-base.csv\")\n",
    "\n",
    "statistics_table = pd.DataFrame([curr_statistics], columns=list(curr_statistics.keys()))\n",
    "statistics_table.to_csv(output_data_directory+\"/alt-statistics-base.csv\")\n",
    "statistics_table.to_excel(statistics_excel_writer, \"base\")\n",
    "\n",
    "#show table\n",
    "display(statistics_table)\n",
    "#plots\n",
    "experiment_plots({'':curr_result})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Neighbors & Distance Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=default_metric\n",
    "nan_filler=default_nan_filler\n",
    "scaler=default_scaler\n",
    "\n",
    "n_neighbors=np.arange(1,11,1)\n",
    "weights=[\"uniform\", \"distance\"]\n",
    "\n",
    "\n",
    "cross_validation = DefaultCrossValidator()\n",
    "y = default_data_scenario_groups\n",
    "\n",
    "curr_data  = default_data_scenario.fillna(nan_filler)\n",
    "\n",
    "# Just a statistics accumulator\n",
    "statistics = []\n",
    "for k in n_neighbors:\n",
    "    for w in weights:\n",
    "        curr_result = knn_experiment_cv(curr_data,\n",
    "                                        cross_validation.split(curr_data[mac_addresses],\n",
    "                                                               y,\n",
    "                                                               groups=default_data_scenario_groups),\n",
    "                                        mac_addresses,\n",
    "                                        coord_cols,\n",
    "                                        scaler=scaler,\n",
    "                                        algorithm=\"brute\",\n",
    "                                        n_neighbors=k,\n",
    "                                        weights=w,\n",
    "                                        metric=metric)\n",
    "        \n",
    "        curr_statistics = experiment_statistics(curr_result)\n",
    "        curr_statistics[\"k\"] = k\n",
    "        curr_statistics[\"weights\"] = w\n",
    "        statistics.append(curr_statistics)\n",
    "    \n",
    "cols = [\"k\",\"weights\"] + list(curr_statistics.keys())[:-2]\n",
    "statistics_table = pd.DataFrame(statistics, columns=cols)\n",
    "statistics_table.to_csv(output_data_directory + \"/alt-statistics-neighbors-weights.csv\")\n",
    "statistics_table.to_excel(statistics_excel_writer, \"neighbors-weights\")\n",
    "\n",
    "#show table\n",
    "display(statistics_table.sort_values(cols[3:]))\n",
    "\n",
    "# Plotting Error statistics\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "index = n_neighbors\n",
    "\n",
    "ax.plot(index, statistics_table[statistics_table[\"weights\"] == \"uniform\"][\"mae\"].tolist(),\n",
    "         color=\"b\", ls=\"-\", label=\"Uniform (MAE)\")\n",
    "\n",
    "ax.plot(index, statistics_table[statistics_table[\"weights\"] == \"distance\"][\"mae\"].tolist(),\n",
    "         color=\"r\", ls=\"-\", label=\"Distance (MAE)\")\n",
    "\n",
    "ax.plot(index, statistics_table[statistics_table[\"weights\"] == \"uniform\"][\"rmse\"].tolist(),\n",
    "         color=\"b\", ls=\"--\", label=\"Uniform (RMSE)\")\n",
    "\n",
    "ax.plot(index, statistics_table[statistics_table[\"weights\"] == \"distance\"][\"rmse\"].tolist(),\n",
    "         color=\"r\", ls=\"--\", label=\"Distance (RMSE)\")\n",
    "\n",
    "ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(MultipleLocator(0.05))\n",
    "\n",
    "ax.set_xlabel(\"Number of Neighbours (k)\")\n",
    "ax.set_ylabel(\"Error (e) in meters (m)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_data_directory+\"/alt-plot-neighbors_weights.pdf\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric\n",
    "Just test a few different distance statistics to assess if there is a better alternative than the plain old *euclidean* distance. The tested statistics include:\n",
    "- Euclidean Distance\n",
    "    - sqrt(sum((x - y)^2))\n",
    "- Manhattan Distance\n",
    "    - sum(|x - y|) \n",
    "- Chebyshev Distance\n",
    "    - sum(max(|x - y|))\n",
    "- Hamming Distance\n",
    "    - N_unequal(x, y) / N_tot\n",
    "- Canberra Distance\n",
    "    - sum(|x - y| / (|x| + |y|))\n",
    "- Braycurtis Similarity\n",
    "    - sum(|x - y|) / (sum(|x|) + sum(|y|))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors=default_n_neighbors\n",
    "weights=default_weights\n",
    "nan_filler=default_nan_filler\n",
    "scaler=default_scaler\n",
    "\n",
    "distance_statistics=[\"euclidean\", \"manhattan\", \"canberra\", \"braycurtis\"]\n",
    "\n",
    "cross_validation = DefaultCrossValidator()\n",
    "y = default_data_scenario_groups\n",
    "\n",
    "curr_data = default_data_scenario.fillna(nan_filler)\n",
    "# Results and statistics accumulators\n",
    "results = {}\n",
    "statistics = []\n",
    "for metric in distance_statistics:\n",
    "    curr_result = knn_experiment_cv(curr_data,\n",
    "                                    cross_validation.split(curr_data[mac_addresses],\n",
    "                                                           y,\n",
    "                                                           groups=default_data_scenario_groups),\n",
    "                                    mac_addresses,\n",
    "                                    coord_cols,\n",
    "                                    scaler=scaler,\n",
    "                                    algorithm=\"brute\",\n",
    "                                    n_neighbors=n_neighbors,\n",
    "                                    weights=weights,\n",
    "                                    metric=metric)\n",
    "    results[metric] = curr_result\n",
    "    curr_statistics = experiment_statistics(curr_result)\n",
    "    curr_statistics[\"metric\"] = metric\n",
    "    statistics.append(curr_statistics)\n",
    "    \n",
    "cols = [\"metric\"] + list(curr_statistics.keys())[:-1]\n",
    "statistics_table = pd.DataFrame(statistics, columns=cols)\n",
    "statistics_table.to_csv(output_data_directory + \"/alt-statistics-metric.csv\")\n",
    "statistics_table.to_excel(statistics_excel_writer, \"metric\")\n",
    "#show table\n",
    "display(statistics_table.sort_values(cols[2:]))\n",
    "#plots\n",
    "experiment_plots(results, \"alt-plot-metric.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "Test different data scaling and normalization approaches to find out if any of them provides a clear advantage over the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_neighbors=default_n_neighbors\n",
    "weights=default_weights\n",
    "metric=default_metric\n",
    "nan_filler=default_nan_filler\n",
    "\n",
    "cross_validation = DefaultCrossValidator()\n",
    "y = default_data_scenario_groups\n",
    "\n",
    "scalers = {\"No Scaling\": None,\n",
    "           \"Rescaling\": preprocessing.MinMaxScaler(),\n",
    "           \"Standardization\": preprocessing.StandardScaler()}\n",
    "# Results and statistics accumulators\n",
    "results = {}\n",
    "statistics = []\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    curr_data = default_data_scenario.fillna(nan_filler)\n",
    "    curr_result = knn_experiment_cv(curr_data,\n",
    "                                    cross_validation.split(curr_data[mac_addresses],\n",
    "                                                           y,\n",
    "                                                           groups=default_data_scenario_groups),\n",
    "                                    mac_addresses,\n",
    "                                    coord_cols,\n",
    "                                    scaler=scaler,\n",
    "                                    algorithm=\"brute\",\n",
    "                                    n_neighbors=n_neighbors,\n",
    "                                    weights=weights,\n",
    "                                    metric=metric)\n",
    "    results[scaler_name] = curr_result\n",
    "    curr_statistics = experiment_statistics(results[scaler_name])\n",
    "    curr_statistics[\"scaler\"] = scaler_name\n",
    "    statistics.append(curr_statistics)\n",
    "\n",
    "cols = [\"scaler\"] + list(curr_statistics.keys())[:-1]\n",
    "statistics_table = pd.DataFrame(statistics, columns=cols)\n",
    "statistics_table.to_csv(output_data_directory + \"/alt-statistics-feature_scaling.csv\")\n",
    "statistics_table.to_excel(statistics_excel_writer, \"feature_scaling\")\n",
    "#show table\n",
    "display(statistics_table.sort_values(cols[2:]))\n",
    "#plots\n",
    "experiment_plots(results, \"alt-plot-feature_scaling.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN filler values\n",
    "Test which is the signal strength value that should be considered for Access Points that are currently out of range. This is needed as part of the process of computing the distance/similarity between different fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors=default_n_neighbors\n",
    "weights=default_weights\n",
    "metric=default_metric\n",
    "scaler=default_scaler\n",
    "\n",
    "min_rssi_value = default_data_scenario[mac_addresses].min().min()\n",
    "nan_fillers = [min_rssi_value,min_rssi_value*1.001,min_rssi_value*1.010,min_rssi_value*1.100,min_rssi_value*1.500]\n",
    "\n",
    "cross_validation = DefaultCrossValidator()\n",
    "y = default_data_scenario_groups\n",
    "\n",
    "# Results and statistics accumulators\n",
    "results = {}\n",
    "statistics = []\n",
    "for nf in nan_fillers:\n",
    "    curr_data = default_data_scenario.fillna(nf)\n",
    "    curr_result = knn_experiment_cv(curr_data,\n",
    "                                    cross_validation.split(curr_data[mac_addresses],\n",
    "                                                           y,\n",
    "                                                           groups=default_data_scenario_groups),\n",
    "                                    mac_addresses,\n",
    "                                    coord_cols,\n",
    "                                    scaler=scaler,\n",
    "                                    algorithm=\"brute\",\n",
    "                                    n_neighbors=n_neighbors,\n",
    "                                    weights=weights,\n",
    "                                    metric=metric)\n",
    "    results[nf] = curr_result\n",
    "    curr_statistics = experiment_statistics(curr_result)\n",
    "    curr_statistics[\"nan_filler\"] = nf\n",
    "    statistics.append(curr_statistics)\n",
    "\n",
    "cols = [\"nan_filler\"] + list(curr_statistics.keys())[:-1]\n",
    "statistics_table = pd.DataFrame(statistics, columns=cols)\n",
    "statistics_table.to_csv(output_data_directory + \"/alt-statistics-nan_filler.csv\")\n",
    "statistics_table.to_excel(statistics_excel_writer, \"nan_filler\")\n",
    "#show table\n",
    "display(statistics_table.sort_values(cols[2:]))\n",
    "#plots\n",
    "experiment_plots(results, \"alt-plot-nan_filler.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search - Automatically searching for the best estimator parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_neighbors_values = range(1,31,1)\n",
    "weights_values = [\n",
    "                    \"uniform\",\n",
    "                    \"distance\"\n",
    "                 ]\n",
    "metric_values = [\n",
    "                    \"euclidean\",\n",
    "                    \"manhattan\",\n",
    "                    \"canberra\", \n",
    "                    \"braycurtis\"\n",
    "                ]\n",
    "algorithm_values = [\"brute\"]\n",
    "\n",
    "nan_filler=default_nan_filler\n",
    "\n",
    "curr_data = default_data_scenario.fillna(nan_filler)\n",
    "\n",
    "param_grid = {\n",
    "                \"kneighborsregressor__n_neighbors\": list(k_neighbors_values),\n",
    "                \"kneighborsregressor__weights\": weights_values,\n",
    "                \"kneighborsregressor__metric\": metric_values,\n",
    "                \"kneighborsregressor__algorithm\": algorithm_values,\n",
    "              }\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "cross_validation = DefaultCrossValidator()\n",
    "y = default_data_scenario_groups\n",
    "\n",
    "estimator = make_pipeline(preprocessing.StandardScaler(), KNeighborsRegressor())\n",
    "\n",
    "grid = GridSearchCV(estimator,\n",
    "                    param_grid=param_grid,\n",
    "                    cv=cross_validation.split(curr_data[mac_addresses], y, groups=default_data_scenario_groups),\n",
    "                    n_jobs=-1,\n",
    "                    scoring=sklearn.metrics.make_scorer(sklearn.metrics.mean_squared_error,\n",
    "                                                        greater_is_better=False,\n",
    "                                                        multioutput=\"uniform_average\"))\n",
    "\n",
    "grid.fit(curr_data[mac_addresses], curr_data[coord_cols], default_data_scenario_groups)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(grid.best_params_)\n",
    "\n",
    "#print(\"Grid scores on development set:\")\n",
    "#gridcv_results = pd.DataFrame(grid.cv_results_)\n",
    "#gridcv_results[['mean_test_score', 'std_test_score', 'params']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of orientation in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_prefixes = [\"left-to-right-point\", \"right-to-left-point\"]\n",
    "filename_prefix_data_scenarios = {}\n",
    "filename_prefix_data_scenarios[\"all\"] = default_data_scenario\n",
    "for filename_prefix in filename_prefixes:\n",
    "    filename_prefix_data_scenarios[filename_prefix] = default_data_scenario[default_data_scenario[\"filename\"].str.startswith(filename_prefix)].reset_index(drop=True)\n",
    "\n",
    "filename_prefix_test_data_scenarios = {}\n",
    "filename_prefix_test_data_scenarios[\"all\"] = test_data_scenario\n",
    "for filename_prefix in filename_prefixes:\n",
    "    filename_prefix_test_data_scenarios[filename_prefix] = test_data_scenario[test_data_scenario[\"filename\"].str.startswith(filename_prefix)].reset_index(drop=True)\n",
    "\n",
    "n_neighbors=default_n_neighbors\n",
    "weights=default_weights\n",
    "metric=default_metric\n",
    "nan_filler=default_nan_filler\n",
    "scaler=default_scaler\n",
    "\n",
    "# Results and statistics accumulators\n",
    "results = {}\n",
    "statistics = []\n",
    "for train_data_keys, train_data in filename_prefix_data_scenarios.items():\n",
    "    for test_data_keys, test_data in filename_prefix_test_data_scenarios.items():\n",
    "        curr_data = train_data.fillna(nan_filler)\n",
    "        curr_test_data = test_data.fillna(nan_filler)\n",
    "        curr_result = knn_experiment(curr_data,\n",
    "                                     curr_test_data,\n",
    "                                     mac_addresses,\n",
    "                                     coord_cols,\n",
    "                                     scaler=scaler,\n",
    "                                     algorithm=\"brute\",\n",
    "                                     n_neighbors=n_neighbors,\n",
    "                                     weights=weights,\n",
    "                                     metric=metric)\n",
    "        label = \"Train: \"+train_data_keys+\" Test: \"+test_data_keys\n",
    "        results[label] = curr_result\n",
    "        curr_statistics = experiment_statistics(curr_result)\n",
    "        curr_statistics[\"orientation\"] = label\n",
    "        statistics.append(curr_statistics)\n",
    "\n",
    "cols = [\"orientation\"] + list(curr_statistics.keys())[:-1]\n",
    "statistics_table = pd.DataFrame(statistics, columns=cols)\n",
    "statistics_table.to_csv(output_data_directory + \"/alt-statistics-orientation.csv\")\n",
    "statistics_table.to_excel(statistics_excel_writer, \"orientation\")\n",
    "#show table\n",
    "display(statistics_table.sort_values(cols[2:]))\n",
    "#plots\n",
    "experiment_plots(results, \"alt-plot-orientation.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of the spacing between reference points in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_reference_points_scenarios = {}\n",
    "coords_indices = default_data_scenario.groupby(coord_cols).indices\n",
    "\n",
    "odd_coords_keys = list(coords_indices.keys())[0::2]\n",
    "odd_ids = []\n",
    "for key in odd_coords_keys:\n",
    "    odd_ids.extend(coords_indices[key])\n",
    "\n",
    "even_coords_keys = list(coords_indices.keys())[1::2]\n",
    "even_ids = []\n",
    "for key in even_coords_keys:\n",
    "    even_ids.extend(coords_indices[key])\n",
    "\n",
    "subset_reference_points_scenarios[\"odd\"] = full_data_scenario.loc[odd_ids].reset_index(drop=True)\n",
    "subset_reference_points_scenarios[\"even\"] = full_data_scenario.loc[even_ids].reset_index(drop=True)\n",
    "subset_reference_points_scenarios[\"all\"] = full_data_scenario\n",
    "\n",
    "n_neighbors=default_n_neighbors\n",
    "weights=default_weights\n",
    "metric=default_metric\n",
    "nan_filler=default_nan_filler\n",
    "scaler=default_scaler\n",
    "# Results and statistics accumulators\n",
    "results = {}\n",
    "statistics = []\n",
    "for train_data_keys, train_data in subset_reference_points_scenarios.items():\n",
    "    curr_data = train_data.fillna(nan_filler)\n",
    "    \n",
    "    cross_validation = LeaveOneGroupOut()\n",
    "    curr_data_scenario_groups = curr_data[\"x\"].map(str)+\",\"+curr_data[\"y\"].map(str)\n",
    "    y = curr_data_scenario_groups\n",
    "    \n",
    "    curr_result = knn_experiment_cv(curr_data,\n",
    "                                    cross_validation.split(curr_data[mac_addresses],\n",
    "                                                           y,\n",
    "                                                           groups=curr_data_scenario_groups),\n",
    "                                    mac_addresses,\n",
    "                                    coord_cols,\n",
    "                                    scaler=scaler,\n",
    "                                    algorithm=\"brute\",\n",
    "                                    n_neighbors=n_neighbors,\n",
    "                                    weights=weights,\n",
    "                                    metric=metric)\n",
    "    results[train_data_keys] = curr_result\n",
    "    curr_statistics = experiment_statistics(curr_result)\n",
    "    curr_statistics[\"reference_points_spacing\"] = train_data_keys\n",
    "    statistics.append(curr_statistics)\n",
    "\n",
    "cols = [\"reference_points_spacing\"] + list(curr_statistics.keys())[:-1]\n",
    "statistics_table = pd.DataFrame(statistics, columns=cols)\n",
    "statistics_table.to_csv(output_data_directory + \"/alt-statistics-reference_points_spacing.csv\")\n",
    "statistics_table.to_excel(statistics_excel_writer, \"reference_points_spacing\")\n",
    "#show table\n",
    "display(statistics_table.sort_values(cols[2:]))\n",
    "#plots\n",
    "experiment_plots(results, \"alt-plot-reference_points_spacing.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of the amount of available data in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors=default_n_neighbors\n",
    "weights=default_weights\n",
    "metric=default_metric\n",
    "nan_filler=default_nan_filler\n",
    "scaler=default_scaler\n",
    "\n",
    "partial_data = [1.0, 0.9, 0.7, 0.5, 0.3, 0.25]\n",
    "repetitions = 5\n",
    "train_data = full_data_scenario[mac_addresses].copy()\n",
    "target_values = full_data_scenario[coord_cols].copy()\n",
    "target_values[\"label\"] = full_data_scenario[\"x\"].map(str) + \",\" + full_data_scenario[\"y\"].map(str)+ \",\" + full_data_scenario[\"filename\"].map(str)\n",
    "\n",
    "# Results and statistics accumulators\n",
    "results = {}\n",
    "statistics = []\n",
    "for partial in partial_data:\n",
    "    curr_result = pd.DataFrame()\n",
    "    for repetition in range(repetitions):\n",
    "        if partial == 1.0:\n",
    "            X_train = train_data\n",
    "            y_train = target_values\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(train_data,\n",
    "                                                                target_values,\n",
    "                                                                test_size=1-partial,\n",
    "                                                                stratify=target_values[\"label\"].values)\n",
    "\n",
    "        train_split_data = pd.concat([y_train, X_train], axis=1).reset_index(drop=True)\n",
    "        curr_data = train_split_data.fillna(nan_filler)\n",
    "        \n",
    "        cross_validation = DefaultCrossValidator()\n",
    "        curr_data_scenario_groups = curr_data[\"x\"].map(str)+\",\"+curr_data[\"y\"].map(str)\n",
    "        y = curr_data_scenario_groups\n",
    "        \n",
    "        curr_result = curr_result.append(knn_experiment_cv(curr_data,\n",
    "                                                           cross_validation.split(curr_data[mac_addresses],\n",
    "                                                                                  y,\n",
    "                                                                                  groups=curr_data_scenario_groups),\n",
    "                                                            mac_addresses,\n",
    "                                                            coord_cols,\n",
    "                                                            scaler=scaler,\n",
    "                                                            algorithm=\"brute\",\n",
    "                                                            n_neighbors=n_neighbors,\n",
    "                                                            weights=weights,\n",
    "                                                            metric=metric), ignore_index=True)\n",
    "    results[partial] = curr_result\n",
    "    curr_statistics = experiment_statistics(curr_result)\n",
    "    curr_statistics[\"partial_data\"] = partial\n",
    "    statistics.append(curr_statistics)\n",
    "\n",
    "cols = [\"partial_data\"] + list(curr_statistics.keys())[:-1]\n",
    "statistics_table = pd.DataFrame(statistics, columns=cols)\n",
    "statistics_table.to_csv(output_data_directory + \"/alt-statistics-partial_data.csv\")\n",
    "statistics_table.to_excel(statistics_excel_writer, \"partial_data\")\n",
    "#show table\n",
    "display(statistics_table.sort_values(cols[2:]))\n",
    "#plots\n",
    "experiment_plots(results, \"alt-plot-partial_data.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save all the data that was collected into an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statistics_excel_writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
